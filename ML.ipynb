{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "moved-compilation",
   "metadata": {},
   "source": [
    "# ML predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-solution",
   "metadata": {},
   "source": [
    "## Import of packages and the preprocessed data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import collections\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "import calendar\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Import the sessions data set\n",
    "path = r'Path file'\n",
    "# For the electronic store data set\n",
    "df = pd.read_csv(path, index_col=None, header=0)\n",
    "\n",
    "#Import the event data set\n",
    "path = r'Path file'\n",
    "# For the electronic store data set\n",
    "dfEvent = pd.read_csv(path, index_col=None, header=0)\n",
    "print(\"Data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-alarm",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df),len(dfEvent))\n",
    "#df = df[df[\"number_of_total_events_per_session_view_cart\"]>1]\n",
    "dfEvent = dfEvent[dfEvent.user_session_id_new.isin(df.user_session_id_new)]\n",
    "print(len(df),len(dfEvent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-greece",
   "metadata": {},
   "source": [
    "## Customization of data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Convert the data types - session dataframe\n",
    "# Convert the session length and max time between two events to seconds\n",
    "df.session_length = pd.DataFrame(df.session_length.map(lambda x: pd.to_timedelta(x).seconds)).values\n",
    "df.max_time_between_two_events = pd.DataFrame(df.max_time_between_two_events.map(lambda x: pd.to_timedelta(x).seconds)).values\n",
    "df['week_of_the_month'] = np.nan\n",
    "df.loc[df['day'] <= 7, 'week_of_the_month'] = 1\n",
    "df.loc[(df['day'] > 7) & (df['day'] <= 15), 'week_of_the_month'] = 2\n",
    "df.loc[(df['day'] > 15) & (df['day']<= 22), 'week_of_the_month'] = 3\n",
    "df.loc[(df['day'] > 22) & (df['day'] <= 31), 'week_of_the_month'] = 4\n",
    "df['month'] = df['month'].apply(lambda x: calendar.month_name[x])\n",
    "df['weekDay'] = df['weekDay'].apply(lambda x: calendar.day_name[x])\n",
    "df = df.drop(columns=[\"day\",\"hour\"])#,\"average_value_of_carts\",'average_value_of_views'\n",
    "df = df.astype({\"user_session_id_new\":'category',\"weekDay\":'category',\"dayTime\":'category',\"PurchaseSession\":'int', \"session_length\":'int64',\"max_time_between_two_events\":'int64',\"month\": 'category','week_of_the_month':\"category\"})\n",
    "df = df.drop([\"event_time\", \"user_id\", \"number_of_purchases\", \"user_session_id_new\"], axis = 1)\n",
    "df = df.rename(columns={'weekDay': 'week_day', 'weekDayOrNot': 'week_day_or_not', \"dayTime\":\"day_time\",\"newVisitorOrNot\":\"new_visitor_or_not\",\"PurchaseSession\":\"purchase_session\"})\n",
    "#################### Convert the data types - event dataframe\n",
    "dfEvent= dfEvent.astype({\"user_id\": 'category',\"user_session_id_new\":'category',\"day\":'int64',\"weekDay\":'category',\"dayTime\":'category', \"product_id\":'category',\"category_code\":'category',\"category_id\":'category',\"brand\":\"category\"})\n",
    "print(\"Converted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-decimal",
   "metadata": {},
   "source": [
    "## Splitting the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-hostel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test set\n",
    "# train set\n",
    "X_train = df.loc[df['set'] == \"train\"].copy(deep=True).drop([\"purchase_session\",\"set\"],axis=1)\n",
    "y_train = df.loc[df['set'] == \"train\"].copy(deep=True).purchase_session\n",
    "# test set\n",
    "X_test = df.loc[df['set'] == \"test\"].copy(deep=True).drop([\"purchase_session\",\"set\"],axis=1)\n",
    "y_test = df.loc[df['set'] == \"test\"].copy(deep=True).purchase_session\n",
    "\n",
    "\n",
    "print(\"Length of training set:\", len(X_train), len(y_train))\n",
    "print(\"Length of test set:\", len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-crawford",
   "metadata": {},
   "source": [
    "## Remove correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check for correlations between features in X_train\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = X_train.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "corr_features = correlation(X_train, 0.9)\n",
    "len(set(corr_features))\n",
    "#print(corr_features)\n",
    "X_train = X_train.drop(corr_features,axis=1)\n",
    "X_test = X_test.drop(corr_features,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-relative",
   "metadata": {},
   "source": [
    "## One hot encoding of categorial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns  = X_train.select_dtypes(include=np.number).columns.to_list()\n",
    "categoricalColumns = X_train.select_dtypes(['category']).columns.to_list()\n",
    "X_train = pd.get_dummies(X_train, columns = categoricalColumns, drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, columns = categoricalColumns, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-sacramento",
   "metadata": {},
   "source": [
    "# Creating prediction models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-estonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_analysis(true_label,predicted):\n",
    "    print(\"Accuracy\",metrics.accuracy_score(true_label, predicted))\n",
    "    print(\"F1 score\",metrics.f1_score(true_label, predicted))    \n",
    "    print(\"Precision score\",metrics.precision_score(true_label, predicted)) \n",
    "    print(\"Recall score\",metrics.recall_score(true_label, predicted))\n",
    "    \n",
    "    \n",
    "def evaluation_roc(true_label,predicted_proba):\n",
    "    print(\"AUC\",metrics.roc_auc_score(true_label, predicted_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-funds",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.logspace(-4, 4, 50)\n",
    "penalty = ['l1', 'l2']\n",
    "sampling_value=(0.2,0.3,0.5)\n",
    "class_weight = [\"balanced\"]\n",
    "params = dict(classification__C= C, classification__penalty=penalty, classification__class_weight = class_weight, sampling__sampling_strategy=sampling_value)\n",
    "\n",
    "model = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        ('sampling', SMOTE()),\n",
    "        ('classification', LogisticRegression(solver='liblinear'))\n",
    "    ])\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "grid = RandomizedSearchCV(model, params, scoring='f1', cv=cv, n_jobs=-1, n_iter=100, random_state=1)\n",
    "result = grid.fit(X_train, y_train)\n",
    "print(\"Logistic regression results:\")\n",
    "print('Best Hyperparameters: %s' % result.best_params_)\n",
    "# Make a prediction on the test set\n",
    "bestParameterLR = result.best_estimator_\n",
    "y_hat = bestParameterLR.predict(X_test)\n",
    "y_hat_proba = bestParameterLR.predict_proba(X_test)\n",
    "evaluation_analysis(y_test, y_hat)\n",
    "evaluation_roc(y_test, y_hat_proba[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-polyester",
   "metadata": {},
   "source": [
    "## Decision tree and random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree\n",
    "criterion = [\"gini\", \"entropy\"]\n",
    "max_features = ['sqrt', 'log2']\n",
    "class_weight = [\"balanced\"]\n",
    "min_samples_leaf = [0.00001, 0.0001, 0.001, 0.01]\n",
    "sampling_value=(0.2,0.3,0.5)\n",
    "\n",
    "params = dict(classification__criterion=criterion, classification__class_weight = class_weight, classification__max_features=max_features,classification__min_samples_leaf=min_samples_leaf, sampling__sampling_strategy=sampling_value)\n",
    "\n",
    "model = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        ('sampling', SMOTE()),\n",
    "        ('classification', DecisionTreeClassifier())\n",
    "    ])\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "grid = RandomizedSearchCV(model, params, scoring='f1', cv=cv, n_jobs=-1, n_iter=100, random_state=1)\n",
    "result = grid.fit(X_train, y_train)\n",
    "print(\"Decision tree results:\")\n",
    "print('Best Hyperparameters: %s' % result.best_params_)\n",
    "# Make a prediction on the test set\n",
    "#bestParameterDT = DecisionTreeClassifier(result.best_params_)\n",
    "bestParameterDT = result.best_estimator_\n",
    "y_hat = bestParameterDT.predict(X_test)\n",
    "y_hat_proba = bestParameterDT.predict_proba(X_test)\n",
    "evaluation_analysis(y_test, y_hat)\n",
    "evaluation_roc(y_test, y_hat_proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "criterion = [\"gini\", \"entropy\"]\n",
    "n_estimators = [100, 200, 300]\n",
    "max_features = ['sqrt', 'log2']\n",
    "min_samples_leaf = [0.00001, 0.0001, 0.001, 0.01]\n",
    "class_weight = [\"balanced\"]\n",
    "sampling_value=(0.2,0.3,0.5)\n",
    "\n",
    "params = dict(classification__criterion=criterion, classification__class_weight = class_weight, classification__n_estimators= n_estimators, classification__max_features=max_features,classification__min_samples_leaf=min_samples_leaf, sampling__sampling_strategy=sampling_value)\n",
    "\n",
    "model = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        ('sampling', SMOTE()),\n",
    "        ('classification', RandomForestClassifier())\n",
    "    ])\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "grid = RandomizedSearchCV(model, params, scoring='f1', cv=cv, n_jobs=-1, n_iter=100, random_state=1)\n",
    "result = grid.fit(X_train, y_train)\n",
    "print(\"Random forest results:\")\n",
    "print('Best Hyperparameters: %s' % result.best_params_)\n",
    "# Make a prediction on the test set\n",
    "bestParameterRF =result.best_estimator_\n",
    "y_hat = bestParameterRF.predict(X_test)\n",
    "y_hat_proba = bestParameterRF.predict_proba(X_test)\n",
    "evaluation_analysis(y_test, y_hat)\n",
    "evaluation_roc(y_test, y_hat_proba[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-lemon",
   "metadata": {},
   "source": [
    "## Gradient boosting machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [100, 200, 300]\n",
    "learning_rate = [0.001, 0.01, 0.1]\n",
    "subsample = [0.5, 0.7, 1.0]\n",
    "max_depth = [2, 3, 4, 6, 8]\n",
    "sampling_value=(0.2,0.3, 0.5)\n",
    "max_features = [\"sqrt\",\"log2\"]\n",
    "\n",
    "params = dict(classification__n_estimators= n_estimators, classification__learning_rate=learning_rate,classification__max_depth=max_depth, classification__subsample=subsample, classification__max_features=max_features, sampling__sampling_strategy=sampling_value)\n",
    "model = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        ('sampling', SMOTE()),\n",
    "        ('classification', GradientBoostingClassifier())\n",
    "    ])\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "grid = RandomizedSearchCV(model, params, scoring='f1', cv=cv, n_jobs=-1, n_iter=3, random_state=1)\n",
    "result = grid.fit(X_train, y_train)\n",
    "print(\"Gradient boosting machine results:\")\n",
    "print('Best Hyperparameters: %s' % result.best_params_)\n",
    "# Make a prediction on the test set\n",
    "bestParameterGB =result.best_estimator_\n",
    "y_hat = bestParameterGB.predict(X_test)\n",
    "y_hat_proba = bestParameterGB.predict_proba(X_test)\n",
    "evaluation_analysis(y_test, y_hat)\n",
    "evaluation_roc(y_test, y_hat_proba[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-satin",
   "metadata": {},
   "source": [
    "## AUC Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUC curves in one plot\n",
    "plt.figure(0).clf()\n",
    "\n",
    "pred = bestParameterLR.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_test, pred)\n",
    "auc = round(metrics.roc_auc_score(y_test, pred),4)\n",
    "plt.plot(fpr,tpr,label=\"Logistic regression, AUC=\"+str(auc))\n",
    "\n",
    "pred =bestParameterDT.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_test, pred)\n",
    "auc =round(metrics.roc_auc_score(y_test, pred),4)\n",
    "plt.plot(fpr,tpr,label=\"Decision tree, AUC=\"+str(auc))\n",
    "\n",
    "\n",
    "\n",
    "pred =bestParameterRF.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_test, pred)\n",
    "auc = round(metrics.roc_auc_score(y_test, pred),4)\n",
    "plt.plot(fpr,tpr,label=\"Random forest, AUC=\"+str(auc))\n",
    "\n",
    "\n",
    "pred =bestParameterGB.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_test, pred)\n",
    "auc = round(metrics.roc_auc_score(y_test, pred),4)\n",
    "plt.plot(fpr,tpr,label=\"Gradient boosting, AUC=\"+str(auc))\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc=0)\n",
    "\n",
    "\n",
    "plt.title('Receiver Operating Characteristic - multicategory data set')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "plt.savefig('File path', format='eps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the validation/test error for different sampling strategies\n",
    "\n",
    "results_cv = pd.DataFrame.from_dict(result.cv_results_, orient='columns')\n",
    "print(results_cv.columns)\n",
    "sns.relplot(data=results_cv ,\n",
    "    kind='line',\n",
    "    x='param_sampling__sampling_strategy',\n",
    "    y='mean_test_score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-crossing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-spine",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
