{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "following-strand",
   "metadata": {},
   "source": [
    "# GNN - item model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-mainland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import collections\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "import calendar\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "#Import the sessions data set\n",
    "path = r'File path'\n",
    "# For the electronic store data set\n",
    "df = pd.read_csv(path, index_col=None, header=0)\n",
    "\n",
    "#Import the event data set\n",
    "path = r'File path'\n",
    "# For the electronic store data set\n",
    "dfEvent = pd.read_csv(path, index_col=None, header=0)\n",
    "print(\"Data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df[\"number_of_total_events_per_session_view_cart\"]>1]\n",
    "df = df[df[\"set\"].notna()]\n",
    "dfEvent = dfEvent[dfEvent.user_session_id_new.isin(df.user_session_id_new)]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Convert the data types - session dataframe\n",
    "# Convert the session length and max time between two events to seconds\n",
    "df.session_length = pd.DataFrame(df.session_length.map(lambda x: pd.to_timedelta(x).seconds)).values\n",
    "df.max_time_between_two_events = pd.DataFrame(df.max_time_between_two_events.map(lambda x: pd.to_timedelta(x).seconds)).values\n",
    "df['week_of_the_month'] = np.nan\n",
    "df.loc[df['day'] <= 7, 'week_of_the_month'] = 1\n",
    "df.loc[(df['day'] > 7) & (df['day'] <= 15), 'week_of_the_month'] = 2\n",
    "df.loc[(df['day'] > 15) & (df['day']<= 22), 'week_of_the_month'] = 3\n",
    "df.loc[(df['day'] > 22) & (df['day'] <= 31), 'week_of_the_month'] = 4\n",
    "df['month'] = df['month'].apply(lambda x: calendar.month_name[x])\n",
    "df['weekDay'] = df['weekDay'].apply(lambda x: calendar.day_name[x])\n",
    "df = df.drop(columns=[\"day\",\"hour\"])#,\"average_value_of_carts\",'average_value_of_views'\n",
    "df = df.astype({\"user_session_id_new\":'category',\"weekDay\":'category',\"dayTime\":'category',\"PurchaseSession\":'int', \"session_length\":'int64',\"max_time_between_two_events\":'int64',\"month\": 'category','week_of_the_month':\"category\"})\n",
    "df = df.drop([\"event_time\", \"user_id\", \"number_of_purchases\"], axis = 1)\n",
    "df = df.rename(columns={'weekDay': 'week_day', 'weekDayOrNot': 'week_day_or_not', \"dayTime\":\"day_time\",\"newVisitorOrNot\":\"new_visitor_or_not\",\"PurchaseSession\":\"purchase_session\"})\n",
    "#################### Convert the data types - event dataframe\n",
    "dfEvent['event_time']= pd.to_datetime(dfEvent['event_time'], infer_datetime_format=True).dt.tz_localize(None)\n",
    "dfEvent= dfEvent.astype({\"newVisitorOrNot\":\"int\", \"user_id\": 'category',\"day\":'category',\"month\":'category',\"user_session_id_new\":'category',\"day\":'int64',\"weekDay\":'category',\"dayTime\":'category', \"product_id\":'category',\"category_code\":'category',\"category_id\":'category',\"brand\":\"category\",\"event_type\":'category',\"user_id\":'category',\"parentProductCategory\":'category',\"secondaryProductCategory\":'category',\"thirdProductCategory\":'category'})\n",
    "dfEvent.weekDayOrNot = dfEvent.weekDayOrNot*1\n",
    "dfEvent = dfEvent.drop(\"user_session\",axis=1)\n",
    "print(\"Converted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding of categorical columns\n",
    "categoricalColumns = dfEvent.select_dtypes(['category']).columns.to_list()\n",
    "#dfEventOneHot = pd.get_dummies(dfEvent, columns = categoricalColumns, drop_first=True)\n",
    "columnsToEncode = ['event_type','weekDay',\"dayTime\"]\n",
    "# Drop all purchase events from the events data frame\n",
    "dfWithoutPurchaseEvents = dfEvent.copy(deep=True)\n",
    "dfWithoutPurchaseEvents = dfWithoutPurchaseEvents.loc[~dfWithoutPurchaseEvents['event_type'].isin(['purchase'])]\n",
    "dfWithoutPurchaseEvents[\"event_type\"] = dfWithoutPurchaseEvents[\"event_type\"].cat.remove_unused_categories()\n",
    "dfWithoutPurchaseEvents['event_type_orig'] = dfWithoutPurchaseEvents['event_type']\n",
    "dfWithoutPurchaseEventsOneHot = pd.get_dummies(dfWithoutPurchaseEvents, columns = columnsToEncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-brunswick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate timedelta between events in a session\n",
    "dfWithoutPurchaseEventsOneHot['sessionTime'] = dfWithoutPurchaseEventsOneHot.sort_values(['user_session_id_new','event_time']).groupby('user_session_id_new')['event_time'].diff().dt.seconds\n",
    "dfWithoutPurchaseEventsOneHot['sessionTime'] = dfWithoutPurchaseEventsOneHot['sessionTime'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dfWithoutPurchaseEventsOneHot[dfWithoutPurchaseEventsOneHot.set==\"train\"]\n",
    "X_test = dfWithoutPurchaseEventsOneHot[dfWithoutPurchaseEventsOneHot.set==\"test\"]\n",
    "\n",
    "# Oversampling of the training set: \n",
    "# Compare the number of purchase and no purchase sessions of the training set and oversample\n",
    "# (duplicate all purchase sessions) to get a ratio thats ca. twice as high as before\n",
    "def oversampling(X_train):\n",
    "    # calculate an appropriate oversample rate\n",
    "    sessions = X_train.drop_duplicates(subset=['user_session_id_new']).reset_index(drop=True)\n",
    "    imbalance_ratio = len(sessions[sessions.PurchaseSession==1])/len(sessions[sessions.PurchaseSession==0])\n",
    "    oversample_rate = max((0,(int(round(1/imbalance_ratio)/8))))#4 bei electronics, 6 cosmetics\n",
    "    print(\"Oversample rate:\", oversample_rate)\n",
    "    # oversample\n",
    "    X_train_original = X_train.copy(deep=True)\n",
    "    if oversample_rate>0:\n",
    "        for i in range(0,oversample_rate):\n",
    "            X_train_purchase = X_train_original[X_train_original[\"PurchaseSession\"]==1].copy(deep=True)\n",
    "            X_train_purchase[\"user_session_id_new\"] = X_train_purchase[\"user_session_id_new\"].astype(str)+str(\"D\")+str(i)\n",
    "            X_train_purchase = X_train_purchase.reset_index(drop=True)\n",
    "            X_train = X_train.append(X_train_purchase)\n",
    "        X_train[\"user_session_id_new\"] = X_train[\"user_session_id_new\"].astype(\"category\")\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the training set old\n",
    "X_train_balanced = oversampling(X_train)\n",
    "# Shuffle the rows\n",
    "X_train_balanced  = X_train_balanced.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "sessions = X_train.drop_duplicates(subset=['user_session_id_new']).reset_index(drop=True)\n",
    "imbalance_ratio = len(sessions[sessions.PurchaseSession==1])/len(sessions[sessions.PurchaseSession==0])\n",
    "print(\"Original class ratio 0:1:\",imbalance_ratio)\n",
    "sessions = X_train_balanced.drop_duplicates(subset=['user_session_id_new']).reset_index(drop=True)\n",
    "imbalance_ratio = len(sessions[sessions.PurchaseSession==1])/len(sessions[sessions.PurchaseSession==0])\n",
    "print(\"Balanced class ratio 0:1:\",imbalance_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the training set NEW\n",
    "# Split X_train in train and validation set\n",
    "X_train_user_sessions = X_train.drop_duplicates(\"user_session_id_new\")[[\"user_session_id_new\",\"PurchaseSession\"]]\n",
    "X_train_uid, X_validation_uid = train_test_split(X_train_user_sessions, test_size=0.3, random_state=42, stratify = X_train_user_sessions.PurchaseSession)\n",
    "X_validation = X_train[X_train.user_session_id_new.isin(X_validation_uid.user_session_id_new.values)]\n",
    "X_train = X_train[X_train.user_session_id_new.isin(X_train_uid.user_session_id_new.values)]\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_validation = X_validation.reset_index(drop=True)\n",
    "\n",
    "# Oversample the train set\n",
    "X_train_balanced = oversampling(X_train)\n",
    "# Shuffle the rows\n",
    "X_train_balanced  = X_train_balanced.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "sessions = X_train.drop_duplicates(subset=['user_session_id_new']).reset_index(drop=True)\n",
    "imbalance_ratio = len(sessions[sessions.PurchaseSession==1])/len(sessions[sessions.PurchaseSession==0])\n",
    "print(\"Original class ratio 0:1:\",imbalance_ratio)\n",
    "sessions = X_train_balanced.drop_duplicates(subset=['user_session_id_new']).reset_index(drop=True)\n",
    "imbalance_ratio = len(sessions[sessions.PurchaseSession==1])/len(sessions[sessions.PurchaseSession==0])\n",
    "print(\"Balanced class ratio 0:1:\",imbalance_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform on the training data \n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_balanced[['sessionTime', 'price','hour']] = min_max_scaler.fit_transform(X_train_balanced[['sessionTime', 'price','hour']])\n",
    "# Transform the test data \n",
    "X_validation[['sessionTime', 'price','hour']] = min_max_scaler.transform(X_validation[['sessionTime', 'price','hour']])\n",
    "X_test[['sessionTime', 'price','hour']] = min_max_scaler.transform(X_test[['sessionTime', 'price','hour']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-comparison",
   "metadata": {},
   "source": [
    "# GNN event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "from sklearn import preprocessing\n",
    "from dgl.data.utils import save_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import DGLDataset\n",
    "class ClickstreamEventDataset(DGLDataset):\n",
    "    def __init__(self, datasetType):\n",
    "        self.datasetType = datasetType\n",
    "        super().__init__(name='synthetic')\n",
    "        \n",
    "\n",
    "    def process(self):\n",
    "        if self.datasetType==\"train\":\n",
    "            data = X_train_balanced.copy(deep=True)#_balanced\n",
    "        elif self.datasetType==\"test\":\n",
    "            data = X_test.copy(deep=True)\n",
    "        elif self.datasetType==\"validation\":\n",
    "            data = X_validation.copy(deep=True)\n",
    "        data.user_session_id_new =data.user_session_id_new.cat.remove_unused_categories()\n",
    "        grouped = data.sort_values(['event_time'],ascending=True).groupby('user_session_id_new')\n",
    "        self.graphs = []\n",
    "        self.labels = []\n",
    "        for user_session_id_new, group in grouped:\n",
    "            \n",
    "            group.reset_index(drop=True, inplace=True)\n",
    "            # Label encode product id, category id, brand\n",
    "            product_id_session, unique = pd.factorize(group[\"product_id\"])\n",
    "            group['product_id_session'] = product_id_session \n",
    "            category_id_session, unique = pd.factorize(group[\"category_id\"])\n",
    "            group[\"category_id_session\"] = category_id_session\n",
    "            brand_session, unique = pd.factorize(group[\"brand\"])\n",
    "            group[\"brand_session\"] = brand_session\n",
    "            \n",
    "         \n",
    "            # Define source and destination nodes\n",
    "            src_nodes = group.product_id_session.values[:-1]\n",
    "            src_nodes = np.append(src_nodes,group.product_id_session.values[1:])\n",
    "\n",
    "            dst_nodes = group.product_id_session.values[1:]\n",
    "            dst_nodes = np.append(dst_nodes,group.product_id_session.values[:-1])\n",
    "            label = group.PurchaseSession.values[0] \n",
    "        \n",
    "            \n",
    "            # Add features\n",
    "            viewsPerProductIdSession = group.groupby([\"product_id_session\"])['event_type_view'].sum().reset_index().event_type_view.values\n",
    "            cartsPerProductIdSession = group.groupby([\"product_id_session\"])['event_type_cart'].sum().reset_index().event_type_cart.values\n",
    "            \n",
    "            #feature_view = group.sort_values('product_id_session').drop_duplicates(\"product_id\").event_type_view.values\n",
    "            #feature_cart = group.sort_values('product_id_session').drop_duplicates(\"product_id\").event_type_cart.values\n",
    "            feature_price = group.sort_values('product_id_session').drop_duplicates(\"product_id\").price.values\n",
    "\n",
    "\n",
    "            # Reset user_session_id_new of oversampled instances\n",
    "            if ('D' in str(user_session_id_new)):\n",
    "                user_session_id_new = str(user_session_id_new).rsplit('D', 1)[0]\n",
    "            feature_session_length =df[df[\"user_session_id_new\"]==user_session_id_new][\"session_length\"].values\n",
    "            #feature_max_time =df[df[\"user_session_id_new\"]==user_session_id_new]['max_time_between_two_events'].values\n",
    "\n",
    "            #event-level features\n",
    "            feature_view = torch.DoubleTensor(viewsPerProductIdSession).unsqueeze(1)\n",
    "            feature_cart = torch.DoubleTensor(cartsPerProductIdSession).unsqueeze(1)\n",
    "            feature_price = torch.DoubleTensor(feature_price).unsqueeze(1)\n",
    "            #feature_product_id = torch.DoubleTensor(feature_product_id).unsqueeze(1) #F.one_hot(torch.DoubleTensor(product_id_session).long())\n",
    "            #feature_category_id  = torch.DoubleTensor(feature_category_id).unsqueeze(1) #F.one_hot(torch.DoubleTensor(category_id_session).long())\n",
    "            #feature_brand  = torch.DoubleTensor(feature_brand).unsqueeze(1) #F.one_hot(torch.DoubleTensor(brand_session).long())\n",
    "            \n",
    "            #graph-level features\n",
    "            feature_session_length = torch.DoubleTensor(feature_session_length).unsqueeze(1)\n",
    "            #feature_max_time = torch.DoubleTensor(feature_max_time).unsqueeze(1)\n",
    "            sessionFeatStack = torch.column_stack((feature_session_length))\n",
    "            \n",
    "            g = dgl.graph((src_nodes, dst_nodes), num_nodes = len(feature_view))\n",
    "            g.ndata['featStack'] = torch.column_stack((feature_view, feature_cart, feature_price))\n",
    "            self.graphs.append(g)\n",
    "            self.labels.append(label)\n",
    "            setattr(g, 'graph_features', sessionFeatStack)\n",
    "        # Convert the label list to tensor for saving.\n",
    "        self.labels = torch.LongTensor(self.labels)    \n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.graphs[i], self.labels[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of graph datasets\n",
    "train_dataset = ClickstreamEventDataset(\"train\")\n",
    "validation_dataset = ClickstreamEventDataset(\"validation\")\n",
    "test_dataset = ClickstreamEventDataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph, label = dataset[5]\n",
    "print(\"Length of training set:\",len(train_dataset))\n",
    "print(\"Length of validation set:\",len(validation_dataset))\n",
    "print(\"Length of test set:\",len(test_dataset))\n",
    "#graph.ndata\n",
    "#print(graph.nodes())\n",
    "#graph.nodes()\n",
    "#graph.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-benefit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calc_auc(predicted_class_proba, labels):\n",
    "    proba = np.concatenate(predicted_class_proba, axis=0)\n",
    "    proba_class1 = pd.Series([item[-1] for item in proba])\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    labels = pd.Series(labels.ravel())\n",
    "    return roc_auc_score(pd.Series(labels.ravel()), proba_class1)\n",
    "\n",
    "def calc_metrics(predicted_class, labels):\n",
    "    predicted_class = np.concatenate(predicted_class, axis=0)\n",
    "    predicted_class = pd.Series(predicted_class.ravel())\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    labels = pd.Series(labels.ravel())\n",
    "    return recall_score(labels, predicted_class),precision_score(labels, predicted_class), f1_score(labels, predicted_class), accuracy_score(labels, predicted_class)\n",
    "\n",
    "\n",
    "def draw_roc(predicted_class_proba, labels):\n",
    "    proba = np.concatenate(predicted_class_proba, axis=0)\n",
    "    proba_class1 = pd.Series([item[-1] for item in proba])\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    labels = pd.Series(labels.ravel())\n",
    "    fpr, tpr, threshold = metrics.roc_curve(labels, proba_class1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    # Plot\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-bibliography",
   "metadata": {},
   "source": [
    "# Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "from dgl.nn import AvgPooling\n",
    "from dgl.nn import MaxPooling\n",
    "from dgl.nn import SumPooling\n",
    "#from dgl.nn import SortPooling\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes, number_of_hiddneurons,dropout_lin):# dropout_lin, dropout_conv):\n",
    "        super(GCN, self).__init__()\n",
    "        allow_zero_in_degree=True\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats+in_feats, h_feats)\n",
    "        self.conv3 = GraphConv(h_feats+in_feats, h_feats)\n",
    "        #self.conv2 = GraphConv(h_feats, h_feats)\n",
    "        #self.conv3 = GraphConv(h_feats, h_feats)\n",
    "        self.conv4 = GraphConv(h_feats+in_feats, h_feats)\n",
    "        self.conv5 = GraphConv(h_feats+in_feats, h_feats)\n",
    "        self.conv6 = GraphConv(h_feats+in_feats, h_feats)\n",
    "        #self.conv6 = GraphConv(h_feats, h_feats)\n",
    "        self.dropout = nn.Dropout(dropout_lin)\n",
    "        \n",
    "        \n",
    "        #self.convadd1 = GraphConv(3, h_feats)\n",
    "        #self.convadd2 = GraphConv(h_feats, h_feats)\n",
    "        #self.convadd3 = GraphConv(h_feats, h_feats)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.avgpooling = AvgPooling()\n",
    "        self.maxpooling = MaxPooling()\n",
    "        self.sumpooling = SumPooling()\n",
    "        #self.dropout = nn.Dropout(dropout_lin)\n",
    "        #self.linear1 = nn.Linear(3*(h_feats+in_feats),  number_of_hiddneurons)\n",
    "        #self.linear1 = nn.Linear(3*(h_feats+in_feats), number_of_hiddneurons)#256\n",
    "        self.linear2 = nn.Linear(3*(h_feats+in_feats)+1, number_of_hiddneurons)#256\n",
    "        #self.linear2 = nn.Linear(number_of_hiddneurons, number_of_hiddneurons)#256\n",
    "        #self.linear3 = nn.Linear (500,200)\n",
    "        self.classify = nn.Linear(number_of_hiddneurons, 2)#num_classes\n",
    "        \n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = F.relu(self.conv1(g, in_feat))\n",
    "        #h = self.dropoutconv(h)\n",
    "        h = torch.cat((h, in_feat), dim=1)\n",
    "        h = F.relu(self.conv2(g, h))\n",
    "        #h = self.dropoutconv(h)\n",
    "        h = torch.cat((h, in_feat), dim=1)\n",
    "        h = F.relu(self.conv3(g, h))\n",
    "        #h = self.dropoutconv(h)\n",
    "        h = torch.cat((h, in_feat), dim=1)\n",
    "        h = F.relu(self.conv4(g, h))\n",
    "        #h = self.dropoutconv(h)\n",
    "        h = torch.cat((h, in_feat), dim=1)\n",
    "        h = F.relu(self.conv5(g, h))\n",
    "        h = torch.cat((h, in_feat), dim=1)\n",
    "        h = F.relu(self.conv6(g, h))\n",
    "        h = torch.cat((h, in_feat), dim=1)\n",
    "        #h = F.relu(self.conv5(g, h))\n",
    "        #h = F.relu(self.conv6(g, h))\n",
    "        #h = torch.cat((h, in_feat), dim=1)\n",
    "        #readout\n",
    "        #g.ndata['h'] = h\n",
    "        #mean = dgl.mean_nodes(g, 'h')\n",
    "        #max_pool = dgl.maxpool(g, 'h')\n",
    "        #h = self.dropout(h)\n",
    "        \n",
    "        #print(in_feat.shape)\n",
    "        \n",
    "        #i = F.relu(self.convadd1(g, in_feat2))\n",
    "        #i = F.relu(self.convadd2(g, i))\n",
    "        #i = F.relu(self.convadd3(g, i))\n",
    "        \n",
    "        #sum2 = self.sumpooling(g, i)\n",
    "\n",
    "        mean = self.avgpooling(g, h)\n",
    "        maxpool = self.maxpooling(g, h)\n",
    "        sumpool = self.sumpooling(g, h)\n",
    "        h = torch.cat((mean, maxpool, sumpool), dim=1)\n",
    "        #h = sumpool\n",
    "        \n",
    "        h = self.dropout(h)\n",
    "        #h = F.relu(self.linear1(h))\n",
    "        h = torch.cat((h,((torch.stack([i.graph_features for i in g.g_list],axis=0).squeeze(1)))), dim=1)\n",
    "        #h = torch.stack([i.graph_features for i in g.g_list],axis=0).squeeze(1)\n",
    "        #h = self.dropout(h)\n",
    "        h = F.relu(self.linear2(h))\n",
    "        #h = self.dropout(h)\n",
    "        #h = F.relu(self.linear3(h))\n",
    "        return self.classify(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _collate(samples):\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs) \n",
    "    batched_graph.g_list = graphs \n",
    "    return batched_graph, torch.tensor(labels)\n",
    "    return _collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-compromise",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "\n",
    "\n",
    "def train_and_validate(param, trial):\n",
    "    \n",
    "    # Create the model with given dimensions\n",
    "    number_of_feats = 4  #3\n",
    "    number_of_classes = 2\n",
    "    number_of_hiddfeat = param['number_of_hiddfeat'] #128\n",
    "    number_of_hiddneurons = param['number_of_hiddneurons']\n",
    "    dropout_lin = param['dropout_lin']\n",
    "\n",
    "        \n",
    "    \n",
    "    model = GCN(number_of_feats, number_of_hiddfeat, number_of_classes, number_of_hiddneurons, dropout_lin)\n",
    "    \n",
    "    batch_size = param['batch_size']\n",
    "    #batch_size = 64\n",
    "    best_vloss_overall = 1_000_000#0\n",
    "    early_stopping = 0\n",
    "\n",
    "\n",
    "    sm = torch.nn.Softmax(dim=1)  \n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "    optimizer = getattr(optim, param['optimizer'])(model.parameters(), lr= param['learning_rate'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=10, threshold=0.001, threshold_mode='abs')\n",
    "    \n",
    "    weight0 = 1 #param['class_weight_zero']\n",
    "    weight1 = param['class_weight_one']\n",
    "    weights = [weight0, weight1]\n",
    "    class_weights=torch.FloatTensor(weights)\n",
    "    loss_function = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    #graphs_train_fold = dgl.data.utils.Subset(train_dataset, train_index)\n",
    "    #graphs_valid_fold = dgl.data.utils.Subset(validation_dataset, test_index)\n",
    "    train_dataloader = GraphDataLoader(train_dataset, batch_size=batch_size, drop_last=False, collate_fn=_collate)\n",
    "    validation_dataloader = GraphDataLoader(validation_dataset,  batch_size=batch_size, drop_last=False, collate_fn=_collate)\n",
    "    epoch_training_losses = []\n",
    "    epoch_validation_losses = []\n",
    "    for epoch in range(1000):\n",
    "        print('\\nEpoch {} '.format(epoch + 1))\n",
    "        \n",
    "        #Training on training set\n",
    "        model.train()\n",
    "        predicted_class_proba = []\n",
    "        predicted_class = []\n",
    "        true = [] #labels\n",
    "        train_loss = 0\n",
    "        num_predictions = 0\n",
    "        #for batched_graph, labels in iter, (bg, label) in train_dataloader:\n",
    "        for iter, (batched_graph, labels) in enumerate(train_dataloader):\n",
    "            \n",
    "\n",
    "            #print(batched_graph.g_list)\n",
    "            #print(batched_graph.ndata['featStack'].float().shape)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            num_predictions += len(labels)\n",
    "            pred = model(batched_graph, batched_graph.ndata['featStack'].float())\n",
    "            probabilities = sm(pred)\n",
    "            predicted_class_proba.append(probabilities.detach().numpy())\n",
    "            true.append(labels.detach().numpy())\n",
    "            predicted_class.append(pred.argmax(1))\n",
    "            #loss = F.cross_entropy(pred, labels)\n",
    "            loss = loss_function(pred,labels)\n",
    "            train_loss += loss.detach().item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # Save the train loss per epoch\n",
    "        train_loss_of_epoch = (train_loss / (iter+1))\n",
    "        recall, precision, f1score, accuracy = calc_metrics(predicted_class, true)\n",
    "        print(\"AUC:\", calc_auc(predicted_class_proba, true))\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"F1-Score:\", f1score)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"--------------------------------------\")   \n",
    "        # Evaluation on validation set\n",
    "        model.eval()\n",
    "        predicted_class_proba = []\n",
    "        predicted_class = []\n",
    "        sm = torch.nn.Softmax(dim=1)\n",
    "        true = [] #labels\n",
    "        validation_loss = 0\n",
    "        num_predictions = 0\n",
    "        for iter, (batched_graph, labels) in enumerate(validation_dataloader): \n",
    "            num_predictions += len(labels)\n",
    "            pred = model(batched_graph, batched_graph.ndata['featStack'].float())\n",
    "            probabilities = sm(pred)\n",
    "            predicted_class_proba.append(probabilities.detach().numpy())\n",
    "            true.append(labels.detach().numpy())\n",
    "            predicted_class.append(pred.argmax(1))\n",
    "            #loss = F.cross_entropy(pred, labels)\n",
    "            loss = loss_function(pred,labels)\n",
    "            validation_loss += loss.detach().item()\n",
    "        # Save the validation loss per epoch\n",
    "        validation_loss_of_epoch = (validation_loss / (iter+1))\n",
    "        recall, precision, f1score, accuracy = calc_metrics(predicted_class, true)\n",
    "        print(\"AUC:\", calc_auc(predicted_class_proba, true))\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"F1-Score:\", f1score)\n",
    "        print(\"Accuracy:\", accuracy)              \n",
    "\n",
    "        roc_auc_score = calc_auc(predicted_class_proba, true)\n",
    "        \n",
    "        # Change learning rate if necessary\n",
    "        #scheduler.step(validation_loss / len(validation_dataloader))\n",
    "        #scheduler.step(validation_loss_of_epoch)\n",
    "        scheduler.step(roc_auc_score)\n",
    "        print(\"LR:\",optimizer.param_groups[0]['lr'])\n",
    "        print('LOSS train: {} LOSS valid: {}'.format(train_loss_of_epoch,validation_loss_of_epoch))\n",
    "\n",
    "        \n",
    "        trial.report(roc_auc_score, epoch)\n",
    "        #epoch_training_losses.append(train_loss_of_epoch)\n",
    "        #epoch_validation_losses.append(validation_loss_of_epoch)\n",
    "        # Track best performance, and save the model's state\n",
    "        if (round(roc_auc_score  ,3) < round(best_vloss_overall,3)):\n",
    "            early_stopping = 0\n",
    "            best_vloss_overall = roc_auc_score #roc_auc_score \n",
    "            model_path_best_overall = 'model_trial_{}'.format(trial.number)\n",
    "            torch.save(model, model_path_best_overall) \n",
    "            \n",
    "        else:\n",
    "            early_stopping += 1\n",
    "            print(\"Epochs without improvement: {}\".format(early_stopping))\n",
    "            if early_stopping > 15:\n",
    "              print(\"Early stopping\")\n",
    "              break\n",
    "        if (round(train_loss_of_epoch,1) == 0):\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    return best_vloss_overall\n",
    "    \n",
    "\n",
    "# Load the saved model\n",
    "#saved_model = GCN(number_of_feats,number_of_hiddfeat,number_of_classes)\n",
    "#saved_model.load_state_dict(torch.load(model_path_best_overall))\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch.optim as optim\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "     params = {\n",
    "              'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n",
    "              'optimizer': trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"]),\n",
    "            'number_of_hiddfeat': trial.suggest_int(\"number_of_hiddfeat\", 32, 256, log =True),\n",
    "            'number_of_hiddneurons': trial.suggest_int(\"number_of_hiddneurons\", 32, 1024, log =True),\n",
    "          #'class_weight_zero': trial.suggest_loguniform(\"class_weight_zero\", 0.1, 1),\n",
    "         'class_weight_one': trial.suggest_loguniform(\"class_weight_one\", 1, 25),\n",
    "         'dropout_lin': trial.suggest_loguniform(\"dropout_lin\", 0.01, 0.5),\n",
    "         'batch_size': trial.suggest_int(\"batch_size\", 128, 512, log=True),\n",
    "\n",
    "     }\n",
    "    \n",
    "     #model = build_model(trial,params)\n",
    "    \n",
    "     best_vloss_overall= train_and_validate(params, trial)\n",
    "\n",
    "     return best_vloss_overall\n",
    "  \n",
    "      \n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=50)\n",
    "study.best_params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Evaluate the best model on the test set\n",
    "# Load the saved model\n",
    "#model = GCN(number_of_feats,number_of_hiddfeat,number_of_classes)\n",
    "#model.load_state_dict(torch.load(str(\"model_epoch_\")+str(study.best_trial.number)))\n",
    "model = torch.load(str(\"model_trial_\")+str(study.best_trial.number))\n",
    "\n",
    "batch_size=256\n",
    "# Load the test set\n",
    "test_dataloader = GraphDataLoader(test_dataset,  batch_size=batch_size, drop_last=False,  collate_fn=_collate)\n",
    "\n",
    "\n",
    "\n",
    "predicted_class_proba = []\n",
    "predicted_class = []\n",
    "sm = torch.nn.Softmax(dim=1)\n",
    "true = [] #labels\n",
    "for batched_graph, labels in test_dataloader:\n",
    "    model.eval()\n",
    "    pred = model(batched_graph, batched_graph.ndata['featStack'].float())\n",
    "    probabilities = sm(pred)\n",
    "    predicted_class_proba.append(probabilities.detach().numpy())\n",
    "    true.append(labels.detach().numpy())\n",
    "    predicted_class.append(pred.argmax(1))\n",
    "\n",
    "recall, precision, f1score, accuracy = calc_metrics(predicted_class, true)\n",
    "print(\"AUC:\", calc_auc(predicted_class_proba, true))\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"F1-Score:\", f1score)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-eligibility",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-frequency",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-mattress",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
